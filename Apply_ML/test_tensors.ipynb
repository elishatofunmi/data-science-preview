{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y =x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/odemakinde/cd/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, shape=(150, 4))\n",
    "y = tf.placeholder(tf.int32,shape = (150,))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(x, feed_dict={x: iris.data, y:iris.target})  # Will succeed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_x = tf.Variable(iris.data, dtype = tf.float32)\n",
    "var_y = tf.Variable(iris.target, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module tensorflow.python.ops.variables:\n",
      "\n",
      "class Variable(builtins.object)\n",
      " |  See the @{$variables$Variables How To} for a high level overview.\n",
      " |  \n",
      " |  A variable maintains state in the graph across calls to `run()`. You add a\n",
      " |  variable to the graph by constructing an instance of the class `Variable`.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable,\n",
      " |  which can be a `Tensor` of any type and shape. The initial value defines the\n",
      " |  type and shape of the variable. After construction, the type and shape of\n",
      " |  the variable are fixed. The value can be changed using one of the assign\n",
      " |  methods.\n",
      " |  \n",
      " |  If you want to change the shape of a variable later you have to use an\n",
      " |  `assign` Op with `validate_shape=False`.\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs for other Ops in the graph. Additionally, all the operators\n",
      " |  overloaded for the `Tensor` class are carried over to variables, so you can\n",
      " |  also add nodes to the graph by just doing arithmetic on variables.\n",
      " |  \n",
      " |  ```python\n",
      " |  import tensorflow as tf\n",
      " |  \n",
      " |  # Create a variable.\n",
      " |  w = tf.Variable(<initial-value>, name=<optional-name>)\n",
      " |  \n",
      " |  # Use the variable in the graph like any Tensor.\n",
      " |  y = tf.matmul(w, ...another variable or tensor...)\n",
      " |  \n",
      " |  # The overloaded operators are available too.\n",
      " |  z = tf.sigmoid(w + y)\n",
      " |  \n",
      " |  # Assign a new value to the variable with `assign()` or a related method.\n",
      " |  w.assign(w + 1.0)\n",
      " |  w.assign_add(1.0)\n",
      " |  ```\n",
      " |  \n",
      " |  When you launch the graph, variables have to be explicitly initialized before\n",
      " |  you can run Ops that use their value. You can initialize a variable by\n",
      " |  running its *initializer op*, restoring the variable from a save file, or\n",
      " |  simply running an `assign` Op that assigns a value to the variable. In fact,\n",
      " |  the variable *initializer op* is just an `assign` Op that assigns the\n",
      " |  variable's initial value to the variable itself.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the variable initializer.\n",
      " |      sess.run(w.initializer)\n",
      " |      # ...you now can run ops that use the value of 'w'...\n",
      " |  ```\n",
      " |  \n",
      " |  The most common initialization pattern is to use the convenience function\n",
      " |  `global_variables_initializer()` to add an Op to the graph that initializes\n",
      " |  all the variables. You then run that Op after launching the graph.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Add an Op to initialize global variables.\n",
      " |  init_op = tf.global_variables_initializer()\n",
      " |  \n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the Op that initializes global variables.\n",
      " |      sess.run(init_op)\n",
      " |      # ...you can now run any Op that uses variable values...\n",
      " |  ```\n",
      " |  \n",
      " |  If you need to create a variable with an initial value dependent on another\n",
      " |  variable, use the other variable's `initialized_value()`. This ensures that\n",
      " |  variables are initialized in the right order.\n",
      " |  \n",
      " |  All variables are automatically collected in the graph where they are\n",
      " |  created. By default, the constructor adds the new variable to the graph\n",
      " |  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n",
      " |  `global_variables()` returns the contents of that collection.\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  between variables holding the trainable model parameters and other variables\n",
      " |  such as a `global step` variable used to count training steps. To make this\n",
      " |  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n",
      " |  `True`, the new variable is also added to the graph collection\n",
      " |  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n",
      " |  `trainable_variables()` returns the contents of this collection. The\n",
      " |  various `Optimizer` classes use this collection as the default list of\n",
      " |  variables to optimize.\n",
      " |  \n",
      " |  @compatibility(eager)\n",
      " |  `tf.Variable` is not compatible with eager execution.  Use\n",
      " |  `tfe.Variable` instead which is compatible with both eager execution\n",
      " |  and graph construction.  See [the TensorFlow Eager Execution\n",
      " |  guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/g3doc/guide.md#variables-and-optimizers)\n",
      " |  for details on how variables work in eager execution.\n",
      " |  @end_compatibility\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = _run_op(a, *args)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float32`, `float64`, `int32`,\n",
      " |          `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\n",
      " |          values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __div__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __floordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `GreaterEqual` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`, `uint32`, `uint64`, `bfloat16`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable.  See ${tf.Tensor$`Tensor.__getitem__`}\n",
      " |      for detailed examples of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.Session() as sess:\n",
      " |        sess.run(tf.global_variables_initializer())\n",
      " |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, or Ellipsis.\n",
      " |  \n",
      " |  __gt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`, `uint32`, `uint64`, `bfloat16`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None, constraint=None)\n",
      " |      Creates a new variable with value `initial_value`.\n",
      " |      \n",
      " |      The new variable is added to the graph collections listed in `collections`,\n",
      " |      which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |      \n",
      " |      If `trainable` is `True` the variable is also added to the graph collection\n",
      " |      `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |      \n",
      " |      This constructor creates both a `variable` Op and an `assign` Op to set the\n",
      " |      variable to its initial value.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, the default, also adds the variable to the graph\n",
      " |          collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as\n",
      " |          the default list of variables to use by the `Optimizer` classes.\n",
      " |        collections: List of graph collections keys. The new variable is added to\n",
      " |          these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device.\n",
      " |          If not `None`, caches on another device.  Typical use is to cache\n",
      " |          on the device where the Ops using the Variable reside, to deduplicate\n",
      " |          copying through `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates\n",
      " |          the Variable object with its contents, referencing the variable's nodes\n",
      " |          in the graph, which must already exist. The graph is not changed.\n",
      " |          `variable_def` and the other arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type.\n",
      " |          If `None`, either the datatype will be kept (if `initial_value` is\n",
      " |          a Tensor), or `convert_to_tensor` will decide.\n",
      " |        expected_shape: A TensorShape. If set, initial_value is expected\n",
      " |          to have this shape.\n",
      " |        import_scope: Optional `string`. Name scope to add to the\n",
      " |          `Variable.` Only used when initializing from protocol buffer.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value\n",
      " |          (which must have the same shape). Constraints are not safe to\n",
      " |          use when doing asynchronous distributed training.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      `tf.Variable` is not compatible with eager execution.  Use\n",
      " |      `tfe.Variable` instead which is compatible with both eager execution\n",
      " |      and graph construction.  See [the TensorFlow Eager Execution\n",
      " |      guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/g3doc/guide.md#variables-and-optimizers)\n",
      " |      for details on how variables work in eager execution.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  __invert__ = _run_op(a, *args)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration. Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __le__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LessEqual` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`, `uint32`, `uint64`, `bfloat16`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`, `uint32`, `uint64`, `bfloat16`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = _run_op(a, *args)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        y: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        y: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __rxor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  __sub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __xor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  assign(self, value, use_locking=False)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the assignment has completed.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the addition has completed.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the subtraction has completed.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See @{tf.Session} for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If\n",
      " |          none, the default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of Variable.shape.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable.\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See @{tf.Session} for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If\n",
      " |            none, the default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Returns the value of this variable, read in the current context.\n",
      " |      \n",
      " |      Can be different from value() if it's on another device, with control\n",
      " |      dependencies, etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False)\n",
      " |      Subtracts `IndexedSlices` from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `scatter_sub(self, sparse_delta.indices,\n",
      " |      sparse_delta.values)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  constraint\n",
      " |      Returns the constraint function associated with this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The constraint function that was passed to the variable constructor.\n",
      " |        Can be `None` if no constraint was passed.\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (tf.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing batch normalizatioin\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden1 = 12\n",
    "n_hidden2 = 24\n",
    "n_outputs = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'x')\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "hidden1 = tf.layers.dense(x, n_hidden1, name = 'hidden1')\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = 'hidden2')\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training = training, momentum = 0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = 'outputs')\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training = training, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.0\n",
      "1 Train accuracy: 0.0\n",
      "2 Train accuracy: 0.0\n",
      "3 Train accuracy: 0.0\n",
      "4 Train accuracy: 0.013333334\n",
      "5 Train accuracy: 0.046666667\n",
      "6 Train accuracy: 0.18\n",
      "7 Train accuracy: 0.23333333\n",
      "8 Train accuracy: 0.24666667\n",
      "9 Train accuracy: 0.25333333\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "epochs = 10\n",
    "\n",
    "test_y = 1.0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(epochs):\n",
    "        sess.run([training_op, extra_update_ops],\n",
    "                feed_dict = {training: True, x: train_x, y: train_y})\n",
    "        correct = tf.nn.in_top_k(logits, train_y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        accuracy_val = accuracy.eval(feed_dict = {x:train_x, y:train_y})\n",
    "        print(epoch, 'Train accuracy:', accuracy_val)\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
